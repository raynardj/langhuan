{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved NER tag loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_data_pytorch(file: Path):\n",
    "    try:\n",
    "        from torch.utils.data.dataset import Dataset\n",
    "        from torch.utils.data.dataloader import DataLoader\n",
    "    except ModuleNotFoundError:\n",
    "        raise ModuleNotFoundError(\n",
    "            \"Please install pytorch first https://pytorch.org/\")\n",
    "    with open(file, \"r\") as f:\n",
    "        json_data = json.loads(f.read())\n",
    "\n",
    "    class NERDataset(Dataset):\n",
    "        \"\"\"\n",
    "        NER Dataset\n",
    "        return text, a list of tags\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "            self.labels = []\n",
    "            for index, labels in data['labels'].items():\n",
    "                self.labels += list(filter(lambda x: \"skipped\" not in x, labels))\n",
    "            self.texts = data['texts']\n",
    "            self.options = data['options']\n",
    "            self.i2c = dict(enumerate([\"O\",]+self.options))\n",
    "            self.c2i = dict((v,k) for k,v in enumerate([\"O\",]+self.options))\n",
    "\n",
    "        def __repr__(self):\n",
    "            options = self.data[\"options\"]\n",
    "            return f\"NERDataset, {options}\\n{len(self)}rows\"\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            label = self.labels[idx]\n",
    "            text = self.texts[str(label[\"index\"])]\n",
    "            return text, label['tags']\n",
    "\n",
    "        def split_train_valid(self, valid_ratio: float = .2):\n",
    "            \"\"\"\n",
    "            split dataset to train and valid\n",
    "            \"\"\"\n",
    "            all_train_index = []\n",
    "            train_texts, val_texts = dict(), dict()\n",
    "            train_labels, val_labels = dict(), dict()\n",
    "\n",
    "            for index, text in self.texts.items():\n",
    "                if random.random() > valid_ratio:\n",
    "                    train_texts.update({index: text})\n",
    "                    all_train_index.append(str(index))\n",
    "                else:\n",
    "                    val_texts.update({index: text})\n",
    "\n",
    "            for index, label_list in self.data[\"labels\"].items():\n",
    "                if index in all_train_index:\n",
    "                    train_labels.update({index: label_list})\n",
    "                else:\n",
    "                    val_labels.update({index: label_list})\n",
    "\n",
    "            train_data, val_data = dict(), dict()\n",
    "\n",
    "            for k, v in self.data.items():\n",
    "                if k == \"texts\":\n",
    "                    train_data.update({\"texts\": train_texts})\n",
    "                    val_data.update({\"texts\": val_texts})\n",
    "                elif k == \"labels\":\n",
    "                    train_data.update({\"labels\": train_labels})\n",
    "                    val_data.update({\"labels\": val_labels})\n",
    "                else:\n",
    "                    train_data.update({k: v})\n",
    "                    val_data.update({k: v})\n",
    "            return self.__class__(train_data), self.__class__(val_data)\n",
    "\n",
    "    return NERDataset(json_data)\n",
    "\n",
    "\n",
    "def load_ner_data_pytorch_huggingface(file, tokenizer, valid_ratio=None):\n",
    "    ner_ds = load_ner_data_pytorch(file)\n",
    "\n",
    "    class NERDatasetHF(Dataset):\n",
    "        def __init__(self, ner_ds, tokenizer):\n",
    "            self.ner_ds = ner_ds\n",
    "            self.options = ner_ds.data[\"options\"]\n",
    "            \n",
    "            self.tokenizer = tokenizer\n",
    "            self.tokenizing = partial(\n",
    "                self.tokenizer,\n",
    "                return_offsets_map=True,\n",
    "                return_tensors=\"pt\")\n",
    "\n",
    "        def __len__(self): return len(self.ner_ds)\n",
    "\n",
    "        def collate(self, batch):\n",
    "            Xs, Ys = zip(*batch)\n",
    "            Xs = list(Xs)\n",
    "            Ys = list(Ys)\n",
    "            \n",
    "            tked = self.tokenizing(Xs)\n",
    "            input_ids = tked['input_ids']\n",
    "            offset_mapping = tked['offset_mapping']\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text, label = self.ner_ds[idx]\n",
    "            return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_ds = load_ner_data_pytorch(\"ner_result_sample.json\")\n",
    "train_ds, val_ds = ner_ds.split_train_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NERDataset, ['school', 'company']\n",
       "2rows"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NERDataset, ['school', 'company']\n",
       "1rows"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, label = train_ds[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_to_offset(tag):\n",
    "    offset = tag['offset']\n",
    "    return [offset, len(tag['text'])+offset],tag['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([122, 144], 'school'), ([346, 354], 'company')]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_pos = list(map(tag_to_offset, label))\n",
    "tags_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.zeros_like(x)\n",
    "\n",
    "tked = tk([text,], return_offsets_mapping = True, return_tensors=\"pt\")\n",
    "om = tked['offset_mapping']\n",
    "x = tked['input_ids']\n",
    "\n",
    "for tag_pos, tag_label in tags_pos:\n",
    "    tag_pos = torch.LongTensor(tag_pos)\n",
    "    tag_mask=(tag_pos[0]<=om[:,0])*(tag_pos[1]>=om[:,1])*train_ds.c2i[tag_label]\n",
    "    y+=tag_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
